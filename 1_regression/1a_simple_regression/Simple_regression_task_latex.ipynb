{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjlEFdXpMgvw"
   },
   "source": [
    "# Deep learning programming I-A: Regression\n",
    "Felix Wiewel, Institute of Signal Processing and System Theory, University of Stuttgart, 08.02.2021\n",
    "###  Implementation\n",
    "\n",
    "In the following we consider a simple regression task, implement a neural network and train it based on the mathematical fomrulation above. For this we first need to create a set of input-output pairs, which then needs to be partitioned into a training, validation and test set. We also define some constants to be used for partitioning the data and the hyperparameters for our neural network.\n",
    "\n",
    "But before we can start, we need to import the necessary packages tensorflow, numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPuVp2lyNK2J"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J84v9uucMgv5"
   },
   "source": [
    "Next we define our constants and set the random seeds of tensorflow and numpy in order to get reproducable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwC-1OnHMgv7"
   },
   "outputs": [],
   "source": [
    "N_train_samples = 600\n",
    "N_validation_samples = 100\n",
    "N_test_samples = 100\n",
    "N_samples = N_train_samples + N_validation_samples + N_test_samples\n",
    "noise_sig = 0.1\n",
    "N_epochs = 150\n",
    "batch_size = 8\n",
    "learning_rate = 0.01\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ngFFSyG-MgwA"
   },
   "source": [
    "We create $600$ training samples, $100$ validation samples to optimize our hyperparameters and $100$ test samples, which are used to check if our model can generalize to unseen data. Furthermore we set the level of noise added to the observations. For training the model we plan to train it for $150$ epochs with a batch size of $8$ and a learning rate of $0.01$. Next we create the actual input-output pairs $\\mathbf{x},\\mathbf{y}$ for which we want to learn the regression model and plot them. In this simple example we choose scalar inputs as well as output but in general $\\mathbf{x}$ and $\\mathbf{y}$ can be vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4s8AtsdQMgwB"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0.0, 3.0, N_samples, dtype=np.float32)\n",
    "y = np.expand_dims(np.sin(1.0+x*x) + noise_sig*np.random.randn(N_samples).astype(np.float32), axis=-1)\n",
    "y_true = np.sin(1.0+x*x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y_true)\n",
    "plt.legend([\"Observation\", \"Ground truth\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3wx8vJlMgwI"
   },
   "source": [
    "With the input-output pairs created, your first task is now to partition the data in the training, validation and test sets. Keep in mind that we have created the data in a structured way, i.e. the input-output pairs are ordered. This means you need to shuffle the data before partitioning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDIMUZs0MgwK"
   },
   "outputs": [],
   "source": [
    "\"\"\" Shuffle and partition the data set accordingly. you can use the predefined constants \"N_train_samples\", \"N_validation_samples\" and \"N_test_samples\". Use the variable names that are already in the below code \n",
    "to store the final shuffled and partitioned data. Hint: Shuffle the data and the labels in such a way that the pairing between an image and it's label is preserved.\"\"\"\n",
    "# Shuffle the data\n",
    "\n",
    "# Partition the data\n",
    "x_train = \n",
    "y_train = \n",
    "x_validation = \n",
    "y_validation = \n",
    "x_test = \n",
    "y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ucvKRhOMgwN"
   },
   "source": [
    "In order to feed the data to our model, we will use the Dataset class provided by Tensorflow. This class is simple to use and provides all the functionality we need for shuffling, batching and feeding the data to our model. It is also tightly integrated into the Tensorflow framework, which makes it very performant. Performance is not an aspect we need to worry about in this exercise, but it is important in more demanding applications.\n",
    "\n",
    "In this exercise we instantiate a separate Dataset object for the training, validation and test data sets, where we shuffle and repeat just the training data set. Shuffling the validation and test data sets is not necessary, since we only evaluate the loss on those data sets and do not perform SGD on it. Please fill in the missing part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HifQ63iPMgwP"
   },
   "outputs": [],
   "source": [
    "\"\"\" Create three tensorflow Dataset objects that can be used to feed the training test and validation data to a neural network. Hint: For the training data set use shuffling, batching with the size according to\n",
    "the predefined constant \"batch_size\" and repeat the data set indefinetly. For the validation and test data sets no shuffling or batching is needed.\"\"\"\n",
    "\n",
    "train_ds = \n",
    "validation_ds = \n",
    "test_ds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Crr6fIkMgwT"
   },
   "source": [
    "In this exercise we will create a a simple neural network with two hidden layers containing $10$ neurons. For creating a model and keeping track of its weights a class called MyModel is used. When initializing an instance of this class the necessary variables are created and stored in a list called \"trainable_variables\". This makes it easy to get all trainable variables of the model. We also override the \\__call__ method of this class in order to implement the forward pass of the neural network. This method should accept the inputs to the neural network and should return the result of the forward pass as an output. Please fill in the missing part of the code and select suitable activation functions for the different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nq8ri416MgwX"
   },
   "outputs": [],
   "source": [
    "\"\"\" Implement a neural network with two hidden dense layers containing 10 neurons each. As an activation function use the tangens hyperbolicus (tf.nn.tanh()). Since we are not using Keras, we need to create and \n",
    "manage all the variables that we need ourselves. The varaibles are created in the constructor of our model class. Since we want to be able to just call the class with some inputs in order to make a prediction, \n",
    "we implement a __call__ method which computes the forward pass and returns the output of the network.\"\"\"\n",
    "\n",
    "class MyModel(object):\n",
    "    def __init__(self):\n",
    "        # Create model variables\n",
    "        self.W0 = \n",
    "        self.b0 = \n",
    "        self.W1 = \n",
    "        self.b1 = \n",
    "        self.W2 = \n",
    "        self.b2 = \n",
    "        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Compute forward pass\n",
    "        output = tf.reshape(inputs, [-1, 1])\n",
    "        output = \n",
    "        output = \n",
    "        output = \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uf6m8zXoMgwb"
   },
   "source": [
    "Now after the model class is defined we can instantiate a MyModel object by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSfI8wjLMgwb"
   },
   "outputs": [],
   "source": [
    "mdl = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KraeH6MsMgwh"
   },
   "source": [
    "We can now use the model to make predictions by calling it. In the following we predict on the inputs an plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1at5RObMgwi"
   },
   "outputs": [],
   "source": [
    "\"\"\" We want to plot a prediction on the complete data set with a model before training. For this make a prediction on the variable \"x\". \"\"\"\n",
    "\n",
    "y_pred = # Compute a prediction on the variable \"x\"\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y_true)\n",
    "plt.plot(x, y_pred.numpy())\n",
    "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utFkv4F3NULQ"
   },
   "source": [
    "Since we have initialized the variables of the neural network randomly, it's prediction is also random. In order to fit the model we need to minimize the expected mean squared error over all input-ouput pairs in our training data set. For this we need to create a function, that performs a training step when provided with the model, an optimizer and a batch of input-ouput pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4zFN0-kOdu7"
   },
   "outputs": [],
   "source": [
    "\"\"\" For training we need to implement a function that executes one training step. Fill in the missing code pieces for this function.\"\"\"\n",
    "\n",
    "def train_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        y_pred = # Compute a prediction with \"model\" on the input \"x\"\n",
    "        loss_val = # Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y\"\n",
    "    grads = tape.gradient(loss_val, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ax-Kfd-tOm7I"
   },
   "source": [
    "This function uses the GradientTape to record the operations for which gradients have to be calculated. In our case this is the forward pass through our model and the computation of the loss function. After these operations are recoded we can get their gradients and apply these through the use of an optimizer. Finally we return the loss value in order to print it.\n",
    "\n",
    "With the training step function defined we now need to choose a suitable optimizer. Tensorflow offers a wide variety of optimizers but in this exercise we will use the RMSprop optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PahsqMscPcKD"
   },
   "outputs": [],
   "source": [
    "opt = tf.optimizers.RMSprop(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_WjHrAwQB9e"
   },
   "source": [
    "We now have everything we need to start training the model. For this we repeatedly sample a batch of input-output pairs from our training data set and use the train_step function to minimize the loss function over this batch. We repeat this until we have iterated over the complete training data set once. After this we compute the loss on the validation data set, print it and repeat with another epoch until we have reached $N\\_epochs$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3LqS3d_QrX6"
   },
   "outputs": [],
   "source": [
    "\"\"\" We can now use the train_step function to perform the training. Fill in the missing code parts.\"\"\"\n",
    "\n",
    "epoch = 0\n",
    "train_iters = 0\n",
    "train_loss = 0.0\n",
    "for x_t, y_t in train_ds:\n",
    "    train_loss += # Perform a training step with the model \"mdl\" and the optimizer \"opt\" on the inputs \"x_t\" and the corresponding targets \"y_t\"\n",
    "    train_iters += 1\n",
    "    if # An epoch is completed\n",
    "        for x_v, y_v in validation_ds:\n",
    "            y_pred = # Compute a prediction with \"mdl\" on the input \"x_v\"\n",
    "            validation_loss = # Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y_v\"\n",
    "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
    "        train_iters = 0\n",
    "        train_loss = 0.0\n",
    "        epoch += 1\n",
    "    if (epoch == N_epochs):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DduaR_pCQ0k-"
   },
   "source": [
    "After completion of the training process we use the test data set to test the models generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ofClNnHRAUy"
   },
   "outputs": [],
   "source": [
    "for x_t, y_t in test_ds:\n",
    "    y_pred = # Compute a prediction with \"mdl\" on the input \"x_t\"\n",
    "    test_loss = # Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y_t\"\n",
    "print(\"Test loss: {:.5}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzJ7wOZmRbez"
   },
   "source": [
    "After we have verified that our model achieves a similar loss on the test as on the validation and training data set, we can conclude that our model is not overfitting or underfitting and generalizes to unseen data. We can now predict on the inputs again and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyvFN03bR8ez"
   },
   "outputs": [],
   "source": [
    "\"\"\" Now we want to plot the prediction after training. Predict on the variable \"x\" again. \"\"\"\n",
    "\n",
    "y_pred = # Compute a prediction on the variable \"x\"\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y_true)\n",
    "plt.plot(x, y_pred.numpy())\n",
    "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKmuAAmLSmiR"
   },
   "source": [
    "Now our model has learned to approximate the function mapping from the input to the output. The capability of neural networks to learn from input-ouput pairs alone and approximate an arbitrary function, see universal approximation theorem, can be very useful if the mapping between the input and output is too complex to be captured with model based approaches. But learning from input-ouput pairs alone implies that the model will only be able to make accurate predictions over input ranges it has seen during training. In order to demonstrate this we will predict on an interval that exeeds the $\\left[0,3\\right]$ interval the model was trained on, i.e. we will predict on the interval $\\left[-2,5\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRF8hAmmVFcH"
   },
   "outputs": [],
   "source": [
    "x_generalize = np.linspace(-2.0, 5.0, N_samples, dtype=np.float32)\n",
    "y_generalize = np.sin(1.0+x_generalize*x_generalize) + noise_sig*np.random.randn(N_samples).astype(np.float32)\n",
    "y_truey_generalize = np.sin(1.0+x_generalize*x_generalize)\n",
    "y_pred = mdl(x_generalize)\n",
    "plt.plot(x_generalize, y_generalize)\n",
    "plt.plot(x_generalize, y_truey_generalize)\n",
    "plt.plot(x_generalize, y_pred.numpy())\n",
    "plt.legend([\"Observation\", \"Target\", \"Prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2yr-0UCVIli"
   },
   "source": [
    "As expected, the model is able to make farely accurate predictions on the interval it was trained on but makes unreliable predictions outside this interval.\n",
    "\n",
    "### Regularization\n",
    "With the theoretical background on regularization we can now implement it and observe it's effects on the regression problem covered in this exercise. For this we will define a model with a high capacity and train it for a extended time to provoke overfitting. For this, we will increase the number of hidden neurons in both hidden layers to $100$ and $50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2Fv2J-VK_vw"
   },
   "outputs": [],
   "source": [
    "\"\"\" Implement a bigger model with again two hidden layers contatining 100 and 50 neurons. As an activation use the tangens hyperbolicus function where it is appropiate. \"\"\"\n",
    "\n",
    "class MyBigModel(object):\n",
    "    def __init__(self):\n",
    "        # Create model variables\n",
    "        self.W0 = \n",
    "        self.b0 = \n",
    "        self.W1 = \n",
    "        self.b1 = \n",
    "        self.W2 = \n",
    "        self.b2 = \n",
    "        self.trainable_variables = [self.W0, self.b0, self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Compute forward pass\n",
    "        output = tf.reshape(inputs, [-1, 1])\n",
    "        output = \n",
    "        output = \n",
    "        output = \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1XpaZTNLV-p"
   },
   "source": [
    "After creating one instance of this class we can again train it on our data set. We will also create a new optimizer for training this bigger model, since some optimizers adapt the learning rates for individual parameters during a training process and we do not want to train our bigger model with learning rates adopted from an earlier training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7Yq-a1FLi0X"
   },
   "outputs": [],
   "source": [
    "big_mdl = MyBigModel()\n",
    "big_opt = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBrf0I68MnMy"
   },
   "source": [
    "Now we are ready to train this bigger model using the same training step and training loop. In order to provoke overfitting we also reduce the number of samples in the training data set a lot, increase the batch size and train for a more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nz_lXM8LMwXo"
   },
   "outputs": [],
   "source": [
    "\"\"\" Implement the training for the bigger model similar to the training of the small model before. \"\"\"\n",
    "\n",
    "N_train_samples_overfit = 30\n",
    "N_epochs = 1000\n",
    "batch_size = 30\n",
    "\n",
    "sel_idx = np.arange(0, N_train_samples)\n",
    "sel_idx = np.random.choice(sel_idx, N_train_samples_overfit)\n",
    "x_train_overfit = x_train[sel_idx]\n",
    "y_train_overfit = y_train[sel_idx]\n",
    "\n",
    "train_overfit_ds = tf.data.Dataset.from_tensor_slices((x_train_overfit, y_train_overfit)).shuffle(N_train_samples_overfit).batch(batch_size).repeat()\n",
    "\n",
    "epoch = 0\n",
    "train_iters = 0\n",
    "train_loss = 0.0\n",
    "for x_t, y_t in train_overfit_ds:\n",
    "    train_loss += # Perform a training step with the model \"big_mdl\" and the optimizer \"big_opt\" on the inputs \"x_t\" and the corresponding targets \"y_t\"\n",
    "    train_iters += 1\n",
    "    if # An epoch is completed\n",
    "        for x_v, y_v in validation_ds:\n",
    "            y_pred = # Compute a prediction with \"big_mdl\" on the input \"x_v\"\n",
    "            validation_loss = # Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y_v\"\n",
    "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
    "        train_iters = 0\n",
    "        train_loss = 0.0\n",
    "        epoch += 1\n",
    "    if (epoch == N_epochs):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkaUDmA8Ps6w"
   },
   "source": [
    "Predicting with this model shows overfitting. For recognizing overfitting a comparison of the validation and training loss is very useful. If the training loss decreases during training while the validation loss consistently increases, the model you are training is probably overfitting. Plotting the models prediction and the target also shows that there is a significant discrepancy between the target and the prediction of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Sq-9xhWPxI4"
   },
   "outputs": [],
   "source": [
    "y_pred = # Predict on x with \"big_mdl\"\n",
    "plt.scatter(x_train_overfit, y_train_overfit)\n",
    "plt.plot(x, y_true)\n",
    "plt.plot(x, y_pred.numpy())\n",
    "plt.legend([\"Target\", \"Prediction\", \"Training samples\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZ36J2EN85b9"
   },
   "source": [
    "In order to implement a regularization we need to modify the loss function. Since the loss function in this exercise is computed during the training step, we define a new training step with a regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sLbcWwlt9Jwl"
   },
   "outputs": [],
   "source": [
    "\"\"\" In order to avoid overfitting we implement a training step that also includes a regularization on the weights of our big model. For this we use the Frobenius/squared l2-norm of each weight matrix/vector. \n",
    "Hint: Use the tf.reduce_sum() function on a list of individual regularization terms for each matrix/vector of the network.\"\"\"\n",
    "\n",
    "def regularized_train_step(model, optimizer, x, y, lmbd):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        y_pred = # Compute a prediction with \"model\" on the input \"x\"\n",
    "        loss_val = # Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y\"\n",
    "        regul_val = # Compute the regularization based on the list \"model.trainable_variables\"\n",
    "        total_loss = # Add the loss with a the regularization term weighted by \"lmbd\"\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ExK4FIw89s0M"
   },
   "source": [
    "We can now set the strength of the regularization and retrain the big model with a regularization. We create another instance of the big model in order to compare the big model with and without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PUUBGi496Vt"
   },
   "outputs": [],
   "source": [
    "\"\"\" Implement the training for the bigger model with the regularized_train_step function. Note: We are plotting the MSE loss without the regularization in order to compare it with the unregularized model. \"\"\"\n",
    "\n",
    "lmbd = 0.005\n",
    "\n",
    "big_reg_mdl = MyBigModel()\n",
    "big_opt = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "epoch = 0\n",
    "train_iters = 0\n",
    "train_loss = 0.0\n",
    "for x_t, y_t in train_overfit_ds:\n",
    "    train_loss += # Perform a regularized training step with the model \"big_mdl\" and the optimizer \"big_opt\" on the inputs \"x_t\" and the corresponding targets \"y_t\" with the regularization parameter being \"lmbd\"\n",
    "    train_iters += 1\n",
    "    if # An epoch is completed\n",
    "        for x_v, y_v in validation_ds:\n",
    "            y_pred = # Compute a prediction with \"big_mdl\" on the input \"x_v\"\n",
    "            validation_loss = # Compute the Mean Squared Error (MSE) for the prediction \"y_pred\" and the targets \"y_v\"\n",
    "        print(\"Epoch: {} Train loss: {:.5} Validation loss: {:.5}\".format(epoch, train_loss/train_iters, validation_loss))\n",
    "        train_iters = 0\n",
    "        train_loss = 0.0\n",
    "        train_reg = 0.0\n",
    "        epoch += 1\n",
    "    if (epoch == N_epochs):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylr1C2ovWXbO"
   },
   "source": [
    "During the training of the regularized model we can already notice, that, although there is still a difference between training and validation loss, the validation loss decreases as the training loss dreases. The effect of the regularization becomes even more evident if we plot the predictions of the regularized model and the overfitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdMRV0vAWLmm"
   },
   "outputs": [],
   "source": [
    "\"\"\" We now want to plot the prediction of the regularized and unregularized big model. \"\"\"\n",
    "\n",
    "y_pred = # Predict with \"big_reg_mdl\" on \"x\"\n",
    "y_pred_overfit = # Predict with \"reg_mdl\" on \"x\"\n",
    "plt.scatter(x_train_overfit, y_train_overfit)\n",
    "plt.plot(x, y_true)\n",
    "plt.plot(x, y_pred.numpy())\n",
    "plt.plot(x, y_pred_overfit.numpy())\n",
    "plt.legend([\"Target\", \"Regularization\", \"No regularization\", \"Training samples\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sePQBaGqZzT6"
   },
   "source": [
    "The model with regularization seems to follow the overall trend of the data, while the model without any regularization very precisely fits the training samples. This is espacilly evident in the interval $\\left[0,0.5\\right]$, where the prediction of the unregularized model shows an oscillating behavior. Such oscillations are however not present in the ground truth and therefore undesirable. The regularized model on the other hand is not as flexible as the unregularized model and therefore does not fit the target function well in the interval $\\left[2.25, 3.0\\right]$.\n",
    "\n",
    "## Conclusion\n",
    "In this exercise we revisited the mathematical background for a simple regression task and covered it's practical implementation in Tensorflow 2. We also explored the phenomenon of overfitting and derived different regularizations from a probabilistic perspective. This exercise covers a very simple task with a very basic neural architecture and is intended as a primer for the second part of the regression exercise, which is dealing with a bigger and more realistic problem. In this second part we will consider the problem of estimating the age of a person from a potrait picture."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Simple_regression_task.ipynb",
   "provenance": [
    {
     "file_id": "1LZtr3XBVqmP7DEYV1lNkLYWlk_Bqu_6f",
     "timestamp": 1562071763633
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
