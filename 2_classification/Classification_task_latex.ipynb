{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Classification_task_latex.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Zafx6Y6AflAu"},"source":["# Deep learning programming II: Classification\n","Felix Wiewel, Institute of Signal Processing and System Theory, University of Stuttgart, 09.02.2021\n","\n","## GPU support\n","In order to speed up calculations with Tensorflow, we need to change the runtime type of this notebook to GPU. For this click on \"Runtime\" in the top left menu and select \"Change runtime type\". Then choose \"GPU\" in the drop down list under \"Hardware accelerator\". This will enable tensorflow to execute calculations on a GPU provided by Google Colab.\n","\n","## Implementation\n","We can simply import all required packages and load some data set using Keras."]},{"cell_type":"code","metadata":{"id":"9Xk5sOe1kLgo"},"source":["import tensorflow as tf\n","import tensorflow.keras as k\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define constants\n","batch_size = 128\n","epochs = 20\n","learning_rate = 0.001\n","\n","(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = tf.keras.datasets.mnist.load_data()\n","x_train_mnist = np.expand_dims(x_train_mnist, axis=-1).astype(np.float32)\n","x_test_mnist = np.expand_dims(x_test_mnist, axis=-1).astype(np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jrs0EEdOknCB"},"source":["We can also plot some examples in order to get an impresion of what the data looks like."]},{"cell_type":"code","metadata":{"id":"iqN-BjZpkxb4"},"source":["plt_img = np.zeros((280, 280))\n","for i in range(10):\n","  for j in range(10):\n","    plt_img[i*28:(i+1)*28, j*28:(j+1)*28] = np.squeeze(x_train_mnist[i*10+j])\n","plt.imshow(plt_img, cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"83RW1--_mTH7"},"source":["Similar to the previous exercises we again derive a class for our model from the Keras Model class."]},{"cell_type":"code","metadata":{"id":"tmk7DlUWoq5X"},"source":["\"\"\" Define a small convolutional network for classification with two conv. and 3 dense layers. The conv. layers should have 8/16 filters a kernel size of 3x3 and a stride of 2. The dense layers should have 128/64/? \n","neurons. Choose the activation functions of the layers accordingly.\"\"\"\n","\n","class MyModel(k.Model):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        # Layer definition\n","\n","    def call(self, inputs, training=False):\n","        # Call layers appropriately to implement a forward pass\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1yVUfCeso4TS"},"source":["We can now instanciate an object of this class and compile it using the the cross-entropy loss function."]},{"cell_type":"code","metadata":{"id":"1-hWI4rVpvdU"},"source":["\"\"\" Instantiate an object of MyModel and an RMSprop optimizer with learning rate given by the constant \"learning_rate\". Compile the model with a suitable loss function and add accuracy as a metric. \"\"\"\n","\n","mdl =\n","opt =\n","mdl.compile()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bzMR_g1-qHTx"},"source":["Now we are ready to train the model and log the metrics for plotting."]},{"cell_type":"code","metadata":{"id":"V1vU1bnnqJZG"},"source":["\"\"\" Train the model mdl on the training data with a batch size of \"batch_size\" for \"epochs\" epochs. Train with 10% of the training data as validation data. \"\"\"\n","\n","history_no_dropout = mdl.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4OhKjUnwtr3M"},"source":["Visualizing the training process in a plot is possible by using the history object that contains a history dictionary. This is returned from the fit function and contains all the metrics logged over the training proces."]},{"cell_type":"code","metadata":{"id":"bPuH5kBQt5yw"},"source":["plt.plot(history_no_dropout.history[\"loss\"])\n","plt.plot(history_no_dropout.history[\"val_loss\"])\n","plt.legend([\"loss\", \"val_loss\"])\n","plt.xticks(range(epochs))\n","plt.xlabel(\"epochs\")\n","plt.title(\"Training process\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bB1KeA-Ww9nE"},"source":["As you can see, the model is clearly overfitting. The loss on the training data is decreasing further and further with every epoch, while the loss on the validation data rises at the same time. We can avoid this by using dropout, which is a strong regularization that you should be familiar with from the lecture. For this we define a new model that has an additional dropout layer with a drop probability of $0.25$ before the first fully connected layer. Otherwise this model is the same as the model above."]},{"cell_type":"code","metadata":{"id":"NowP4zSsxfhe"},"source":["\"\"\" Create a new model, which is identical to MyModel execpt for a dropout layer between the conv. and dense layers. As a dropout rate use 0.25 \"\"\"\n","\n","class MyDropoutModel(k.Model):\n","    def __init__(self):\n","        super(MyDropoutModel, self).__init__()\n","        # Layer definition\n","\n","    def call(self, inputs, training=False):\n","        # Call layers appropriately in order to implement the forward pass\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XivWPeUxphB"},"source":["We can now create an instance of this model, train it and visualize the training process."]},{"cell_type":"code","metadata":{"id":"VZxNQX2Bxtno"},"source":["\"\"\" Instantiate a MyDropoutModel object, compile and train it on the training data. Use the same optimizer and parameters for training as before. \"\"\"\n","\n","dropout_mdl = \n","dropout_opt = \n","dropout_mdl.compile()\n","\n","history_dropout = dropout_mdl.fit()\n","\n","plt.plot(history_no_dropout.history[\"loss\"])\n","plt.plot(history_no_dropout.history[\"val_loss\"])\n","plt.plot(history_dropout.history[\"loss\"])\n","plt.plot(history_dropout.history[\"val_loss\"])\n","plt.legend([\"loss\", \"val_loss\", \"loss w. dropout\", \"val_loss w. dropout\"])\n","plt.xticks(range(epochs))\n","plt.xlabel(\"epochs\")\n","plt.title(\"Training process\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bKRoJPWNNBxS"},"source":["While the model without dropout can reach a much lower loss on the training data, it generalizes very poorly to the unseen validation data. The model with dropout however generalizes quite good to the validation data and achieves a similar loss on both the data used during training and unseen data. This demonstrates that using dropout can be a good way to regularize your networks and prevent overfitting. Although dropout helps to prevent overfitting very well, it introduces another hyperparameter, the drop probability, that needs to be optimized. Common values for this hyperparameter are on the interval $\\left[0.2,0.5\\right]$.\n","\n","## Transfer Learning\n","In this exercise, we will also use a neural network pretrained on ImageNet in order to fine tune it for the [Caltech 101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) data set. For a guide on transfer learning using fine tuning with Keras click [here](https://www.tensorflow.org/beta/tutorials/images/transfer_learning).\n","\n","In order to do this we first need to download an extract the data set. Note that there is a wide collection of data sets available through the [tensorflow_datasets](https://www.tensorflow.org/datasets) package including Caltech101. But for demonstration purposses we manually download and load the images using Keras."]},{"cell_type":"code","metadata":{"id":"WZLz-aD6tgvR"},"source":["!gdown https://drive.google.com/u/0/uc?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp&export=download\n","!tar -xzf 101_ObjectCategories.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4YVR0yd-3A3w"},"source":["In order to feed our model with the data, we will use the ImageDataGenerator class provided by Keras. This class can be used for reading files from a structured directory, create the labels based on the structure of the directory and apply data augmentation techniques. The ImageDataGenerator already supports a lot of techniques for data augmentation. In our example, we use a random rotation, width shift, height shift, shearing, zomming and horizontal flipping. All of those operations are applied randomly to individual images. More information on the available transformations for data augmentation and how to use them see the [documentation](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#__init__)."]},{"cell_type":"code","metadata":{"id":"WueTCO3F3iKl"},"source":["N_samples_Caltech101 = 9144\n","val_split = 0.1\n","datagen = k.preprocessing.image.ImageDataGenerator(validation_split=val_split,\n","                                                   preprocessing_function=k.applications.mobilenet_v2.preprocess_input,\n","                                                   rotation_range=20,\n","                                                   width_shift_range=0.1,\n","                                                   height_shift_range=0.1,\n","                                                   shear_range=0.1,\n","                                                   zoom_range=0.1,\n","                                                   horizontal_flip=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PyqbHZfO0ein"},"source":["Now that we have the data set we can load the pretrained model. Keras provides a range of pretrained models called \"applications\", click [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications) for a link to its documentation. We will use the MobileNetV2 architecture without it's output layer, since we want to modify it in order to apply the model to the Caltech 101 data set."]},{"cell_type":"code","metadata":{"id":"ZkGHDLKZ0uUy"},"source":["\"\"\" Instantiate a MobileNetV2 with weights pretrained on ImageNet and without the top/output layer. Hint: Use Keras Applications\"\"\"\n","\n","base_model = \n","base_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ngVtdWor2z62"},"source":["With the pretrained model we can build our actual model that adds an output layer to the MobileNetV2."]},{"cell_type":"code","metadata":{"id":"HQNsTLZI4GOW"},"source":["\"\"\" Create a transfer learning model, that uses a pretrained model \"pretrained_model\" and appends a 2D global average pooling layer, a dropout layer (droprate 0.25) and a dense output layer with 102 neurons. \"\"\"\n","\n","class MyTransferModel(k.Model):\n","    def __init__(self, pretrained_model):\n","        super(MyTransferModel, self).__init__()\n","        # Define layers and pretrained model\n","\n","    def call(self, inputs, training=False):\n","        # Call pretrained model and layers appropriately to implement forward pass\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w5w6aV4S4nHu"},"source":["Now we just need to instantiate our model for transfer learning and fine tune it. But instead of directly training all layers we will just train the last layer first. If you do not do this and directly train the complete model, the random initialization of the last layer can cause gradients with very big magnitude that will be propageted into the MobileNetV2 layers and cause them to \"forget\" what they have learned on ImageNet. This is undesireable since we want to transfer that knowledge over into our model in order to achieve better performance on the Caltech 101 data set."]},{"cell_type":"code","metadata":{"id":"ayjQnTqz47CO"},"source":["\"\"\" Instantiate a MyTransferModel object and a RMSprop optimizer, compile them with a suitable loss and accuracy as a metric. Use \"base_model\" as the pretrained model. \"\"\"\n","\n","tf_batch_size = 32\n","tf_epochs = 10\n","tf_learning_rate = 0.001\n","tf_mdl =\n","tf_opt =\n","base_model.trainable = False\n","tf_mdl.compile()\n","tf_mdl.build((tf_batch_size, 224, 224, 3))\n","tf_mdl.summary()\n","\n","\"\"\" Create the data set generators and train the model for \"tf_epochs\" epochs. Hint: Use the class mode \"sparse\" and the appropriate subsets for creating the generators and \n","steps_per_epoch=int((1.0-val_split)*N_samples_Caltech101/tf_batch_size) as well as a suitable number of validation_steps for the fit function. The data for the Caltech101 \n","data set is located in the \"101_ObjectCategories\" directory. \"\"\"\n","\n","train_gen = datagen.flow_from_directory()\n","val_gen = datagen.flow_from_directory()\n","tf_history_0 = tf_mdl.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HbKiWk_T4zH1"},"source":["Now that we have trained the outputlayer of our model on the Caltech 101 data set, we can make the last layers of the MobileNetV2 model trainable and continue to fine tune  it with a low learning rate. For this we need to recompile our model in order for the change of the MobileNetV2 parameters to trainable to have an effect."]},{"cell_type":"code","metadata":{"id":"yNL1HtEp5R2W"},"source":["\"\"\" Reinstantiate the RMSprop optimizer with the changed learning rate and set the base_model to be trainable. Then compile it with the newly instantiate optimizer, a suitable loss function and accuracy as a metric \n","and continue training on the Caltech101 data set. Hint: Angain use steps_per_epoch=int((1.0-val_split)*N_samples_Caltech101/tf_batch_size) as well as a suitable number of validation_steps to train the model fo\n","\"tf_epochs\" epochs on the data set. \"\"\"\n","\n","tf_learning_rate = 0.00001\n","tf_opt = \n","# Set base_model to be trainable\n","tf_mdl.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf_opt, metrics=[\"accuracy\"])\n","tf_mdl.build((tf_batch_size, 224, 224, 3))\n","tf_mdl.summary()\n","\n","tf_history_1 = tf_mdl.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNo2frodRH_i"},"source":["As you can see from the increasing gap between training and validation loss and accuracy, overfitting is becoming a bigger problem now. This is due to the much higher number of trainable parameters if we not only train the output layer on the rather small Caltech 101 data set. But still we can improve the performance of our model. We can visualize this if we plot the accuracy over the training process."]},{"cell_type":"code","metadata":{"id":"g2QHrVsvRv0C"},"source":["plt.plot(tf_history_0.history[\"accuracy\"]+tf_history_1.history[\"accuracy\"])\n","plt.plot(tf_history_0.history[\"val_accuracy\"]+tf_history_1.history[\"val_accuracy\"])\n","plt.xticks(range(len(tf_history_0.history[\"accuracy\"]+tf_history_1.history[\"accuracy\"])))\n","plt.axvline(9, color=\"green\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"Acc\", \"Val. Acc\", \"Start fine tuning\"])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2Jhnwh5Y4IB"},"source":["Overall the resulting performance of the model is quite close to published results on the Caltach 101, e.g. this [paper](https://arxiv.org/abs/1406.4729v1), while the definintion and training of the model only required very little code.\n","\n","In order to evaluate if transfer learning actually makes a difference, we can simply train the MobileNetV2 on the Caltech 101 data set. As we already discussed, training such a high capacity model on such a small data set leads to heavy overfitting."]},{"cell_type":"code","metadata":{"id":"Ugaum5NNdxb1"},"source":["\"\"\" Instantiate a new base_model, now with randomly initialized weights, and instantiate a new MyTransferModel object. Compile it with a RMSprop optimizer, a suitable loss and accuracy as a metric.  \"\"\"\n","\n","tf_batch_size = 32\n","tf_epochs = 5\n","tf_learning_rate = 0.001\n","base_model = # Instantiate a randomly initialized MobileNetV2 without it's top/output layer\n","tf_mdl =\n","tf_opt =\n","tf_mdl.compile()\n","tf_mdl.build((tf_batch_size, 224, 224, 3))\n","tf_mdl.summary()\n","\n","\"\"\" Train this newly instantiated model on the Caltech101 data set. Hint: Again use steps_per_epoch=int((1.0-val_split)*N_samples_Caltech101/tf_batch_size) as well as a suitable number of validation_steps. \"\"\"\n","\n","tf_history_2 = tf_mdl.fit()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7gvjWFvtlxr"},"source":["As you can see, training MobileNetV2 from scratch does not only suffer from severe overfitting but is also rather slow when compared with fine tuning. So overall it is recommended to use pretrained models and transfer learning if you want to train a high capacity neural network on a small data set, as it is often the case in practical applications. By doing this you can benefit from not only the architecture search of many experts but also save time and computational resources since a part of your model has already be pretrained by someone else. But you should always keep in mind that transfer learning is only a suitable method if there is a realistic chance of a positive transfer, i.e. the data set and task the model was pretrained on has to have something in common with your data set and task. Otherwise transfer learning can not only slow down the convergence speed but also hurt the final performance of your model."]},{"cell_type":"markdown","metadata":{"id":"3zzJJ51h2GP8"},"source":["## Catastrophic Forgetting\n","In order to demonstrate catastrophic forgetting, we will train a neural network first on MNIST and then on the FashionMNIST data set. We want to be able to uniquely identify any class from both data sets. Since both data sets contain $10$ classes and use the labels $\\lbrace0,\\ldots,9\\rbrace$ for them, we need to choose different labels for the FashionMNIST data set. We do this by loading the data set and shifting the labels by $10$ so that we get labels $\\lbrace10,\\ldots,19\\rbrace$."]},{"cell_type":"code","metadata":{"id":"x0e8W8Wc3CS_"},"source":["\"\"\" Shift the labels of the FashionMNIST data set to {10,...,19}. \"\"\"\n","\n","(x_train_fmnist, y_train_fmnist), (x_test_fmnist, y_test_fmnist) = tf.keras.datasets.fashion_mnist.load_data()\n","\n","x_train_fmnist = np.expand_dims(x_train_fmnist, axis=-1).astype(np.float32)\n","y_train_fmnist = # Shift labels of training data\n","x_test_fmnist = np.expand_dims(x_test_fmnist, axis=-1).astype(np.float32)\n","y_test_fmnist = # Shift labels of testing data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fz6e_B1vFj96"},"source":["We also plot some examples of this data set and varify if the labels were shifted correctly."]},{"cell_type":"code","metadata":{"id":"bZkh_WV-FqD_"},"source":["plt_img = np.zeros((280, 280))\n","for i in range(10):\n","  for j in range(10):\n","    plt_img[i*28:(i+1)*28, j*28:(j+1)*28] = np.squeeze(x_train_fmnist[i*10+j])\n","plt.imshow(plt_img, cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.show()\n","\n","print(\"Labels\")\n","print(\"MNIST: \"+str(np.unique(y_test_mnist)))\n","print(\"FashionMNIST: \"+str(np.unique(y_test_fmnist)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W6SBIqCcOdWe"},"source":["Now the data is prepared we need to define a new model that can classify images into $20$ different classes. This is necessary since the models we used up to this point only have $10$ neurons in their output layers and therefore are only capable of classifying into $10$ different categories."]},{"cell_type":"code","metadata":{"id":"8WC9fu-hOzVH"},"source":["\"\"\" Implement a new model, which is capable of classifying 20 classes. Use two conv. layers with 8/16 filters of size 3x3 and a stride of 2, a dropout layer between the conv. and dense layers with a droprate of 0.25 \n","three dense layers with 128/64/? neurons. Choose all activation functions appropriately.\"\"\"\n","\n","class MyExtendedModel(k.Model):\n","    def __init__(self):\n","        super(MyExtendedModel, self).__init__()\n","        # Layer definition\n","\n","    def call(self, inputs, training=False):\n","        # Call layers in correct order to implement forward pass\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a9xyVgZrQNjg"},"source":["With this model we are almost redy to start training on a sequence of tasks, i.e. we will first train on the MNIST data set in order to learn the classes $0$ up to $9$ and after that train on the FshionMNIST data set in order to learn the remaining classes from $10$ to $19$. During this process we want to evaluate the model separately on MNIST and FashionMNIST validation data. Since this is not a standard procedure, we need to implement a Callback class. Callbacks in Keras are used to implement actions that are executed at different points during the training process, e.g. at the beginning of the training, after a batch is processed or at the end of each epoch. See the [documentation](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/callbacks) for an overview of all provided callbacks and the [guide](https://www.tensorflow.org/beta/guide/keras/custom_callback#an_overview_of_callback_methods) on writing custom callbacks. While there are predefined callbacks, e.g. for logging metrics into a tensorboard log file, there is unfortunately no callback that we could use to evaluate our model simultaneously on two different validation data sets. The only option we have is to implement our own callback."]},{"cell_type":"code","metadata":{"id":"7bOiLxIRSCz5"},"source":["\"\"\" Implement a custom callback that evaluates a model on two data sets at the end of an epoch and stores the results in a two separate lists. Hint: The Keras callback class always posses an assotiated model. You can \n","use it via the \"self.model\" attribute of the class. Hint: Evaluating the model will return a tuple containing two elements, i.e. (loss, acc).  \"\"\"\n","\n","class MyCallback(tf.keras.callbacks.Callback):\n","  # Get the two different data sets and create lists for storing results\n","  def __init__(self, x_0, y_0, x_1, y_1, batch_size):\n","    super(MyCallback, self).__init__()\n","    self.x_0 =\n","    self.y_0 =\n","    self.x_1 =\n","    self.y_1 =\n","    self.loss_0 =\n","    self.acc_0 =\n","    self.loss_1 =\n","    self.acc_1 =\n","    self.batch_size =\n","    \n","  def on_epoch_end(self, epoch, logs=None):\n","    # Evaluate the model on both data sets and store results\n","    print(\"\\nStarting callback...\")\n","    print(\"+----------------------+\")\n","    print(\"| Data set 0           |\")\n","    print(\"+----------------------+\")\n","    metrics_0 = #Evaluate the model on \"self.x_0\" and \"self.y_0\" with \"self.batch_size\"\n","    # Append loss to the loss list \"self.loss_0\" and accuracy to the accuracy list \"self.acc_0\"\n","    print(\"+----------------------+\")\n","    print(\"| Data set 1           |\")\n","    print(\"+----------------------+\")\n","    metrics_1 = #Evaluate the model on \"self.x_1\" and \"self.y_1\" with \"self.batch_size\"\n","    # Append loss to the loss list \"self.loss_1\" and accuracy to the accuracy list \"self.acc_1\"\n","    print(\"Callback completed...\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h3ZUw3WqWmSL"},"source":["This callback will accept two data sets on its initialization that can be used during training to evaluate on. For this we override the on_epoch_end function to evaluate on both data sets, print the results and store them for later use. We now train our neural network first on MNIST in order to learn the classes $\\lbrace0,\\ldots,9\\rbrace$ and then on FashionMNIST in order to learn the classes $\\lbrace10,\\ldots,19\\rbrace$  while simultaneously evaluating the model on both, the MNIST and FashionMNIST test sets. If our neural network would be capable of learning continually, we would expect to see the loss on MNIST decrease as we train on it and remain low as we continue with the training on FashionMNIST. Similarly we would expect the accuracy to rise on MNIST as we train on it and remain stable even if we continue to train on FashionMNIST."]},{"cell_type":"code","metadata":{"id":"n3tZfi6jVSec"},"source":["\"\"\" Instantiate a MyExtendedModel object, a RMSprop optimizer with learning rate \"learning_rate\" and compile them with a suitable loss function and accuracy as a metric. Then instantiate a MyCallback object with the \n","MNIST test data set \"x_test_mnist\", \"y_test_mnist\" and the FashionMNIST test data set \"x_test_fmnist\", \"y_fmnist\". Finally train the model first on the MNIST training data set and then on the FashionMNIST data set. \n","Hint: Use the MyCallback object \"my_cb\" during the training in order to record the accuracies on both data sets during training. \"\"\"\n","\n","extended_mdl =\n","extended_opt =\n","extended_mdl.compile()\n","\n","my_cb = \n","\n","extended_mdl.fit() # Train on MNIST\n","extended_mdl.fit() # Train on FashionMNIST"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaUL-QYT8tfy"},"source":["If we now plot the loss on the test sets during the sequential training process on MNIST and FashionMNIST, we can clearly see what catastrophic forgetting means."]},{"cell_type":"code","metadata":{"id":"vziuxbrI9NLF"},"source":["plt.subplot(2, 1, 1)\n","plt.plot(my_cb.loss_0)\n","plt.plot(my_cb.loss_1)\n","plt.legend([\"MNIST\", \"FashionMNIST\"])\n","plt.title(\"Loss\")\n","plt.subplot(2, 1, 2)\n","plt.plot(my_cb.acc_0)\n","plt.plot(my_cb.acc_1)\n","plt.legend([\"MNIST\", \"FashionMNIST\"])\n","plt.title(\"Accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSfJAOLbBsv3"},"source":["We can clearly see the transition from training on MNIST to training on FashionMNITS on epoch $20$. At this epoch the loss on MNIST rapidly increases again while the loss on FashionMNIST rapidly decreases. For the accuracy we see a similar but opposite behaivior. It first is high while training on MNIST and then rapidly decreases towards $0$ as we transition to training on FashionMNIST. Avioding this phenomenon is an active area of research on deep learning and there already exist some methods in order to mitigate catastrophic forgetting. But so far solving continual learning with neural networks remains an interesting and challenging problem. If it can be solved, neural networks could learn more human like without a constant need to refresh previously learned examples and enable interesting new applications where knowledge is accumulated over time.\n","\n","## Conclusion\n","In this exercise we have learned how to use Keras in order to quickly develop neural networks and train them on different data sets. We also refreshed the mathematical background on classification and explored how dropout can act as an effective regularization in order to mitigate overfitting. While the Keras specification offers a high-level of abstraction in order to simplify build and training neural networks, it also enables us to customize some parts of the complete workflow. In order to evaluate a model on two different data sets in order to visualize the phenomenon of catastrophic forgetting, we implemented a custom callback that executed our required action at the end of every epoch. Besides that Keras offers many other ways to implement non standard layers/methods. If you want to learn more on Keras, you can visit the documentation on the Keras implementation in Tensorflow [here](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras) or directly use the Keras specification available [here](https://keras.io). Additionally we explored a very basic transfer learning technique, i.e. fine tuning of pretrained models. For this we utilized the MobileNetV2 architecture that was trained on the ImageNet data set and fine tuned it on the Caltech 101 data set. The resulting model was simple to implement and fast to train, while it still achieved competetive results."]}]}